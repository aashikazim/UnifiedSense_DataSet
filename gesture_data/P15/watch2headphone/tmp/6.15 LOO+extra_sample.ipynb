{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178acef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.all import *\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995b998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model(X_train, X_test, y_train, y_test, epoch):\n",
    "    X, y, splits = combine_split_data([X_train, X_test], [y_train.ravel(), y_test.ravel()])\n",
    "    tfms  = [None, [Categorize()]]\n",
    "    dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
    "    \n",
    "    ###fit\n",
    "    dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=64, batch_tfms=TSStandardize(by_var=True))\n",
    "    model = TST(dls.vars, dls.c, dls.len, dropout=0.4, fc_dropout=0.9)\n",
    "    learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropyFlat(), \n",
    "                    metrics=accuracy,  cbs=ShowGraphCallback2())\n",
    "    learn.fit_one_cycle(epoch, 1e-4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    acc, f1 = calculate_acc_f1(learn, X_test, y_test)\n",
    "    return learn, acc, f1\n",
    "\n",
    "\n",
    "def calculate_acc_f1(learn, X_test, y_test):\n",
    "    probas, target, preds = learn.get_X_preds(X_test, y_test.ravel(), with_decoded = True)\n",
    "    k = torch.argmax(probas, dim = 1)\n",
    "    acc = accuracy_score(y_test.ravel(), k.tolist())\n",
    "    f1 = f1_score(y_test.ravel(), k.tolist(), average='weighted')\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a2702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1744, 1)\n"
     ]
    }
   ],
   "source": [
    "base_path = \"UnifiedSense/np_data\"\n",
    "folders = [\"/ring2glass/\", \"/ring2headphone/\", \"/ring2watch/\", \"/watch2glass/\", \"/watch2headphone/\"]\n",
    "N = 12\n",
    "epoch = 20\n",
    "total_acc = 0\n",
    "total_f1 = 0\n",
    "final_record_acc = []\n",
    "final_record_f1 = []\n",
    "max_accuracy = 0\n",
    "for k in range(N-1, N):\n",
    "    record_acc = []\n",
    "    record_f1 = []\n",
    "    test_data = np.load(\"data_dump/\"+\"LOO_ALL_TEST_DATA\"+str(k+1)+\".npy\", mmap_mode='c')\n",
    "    test_label = np.load(\"data_dump/\"+\"LOO_ALL_TEST_LABEL\"+str(k+1)+\".npy\", mmap_mode='c')\n",
    "    train_data = np.load(\"data_dump/\"+\"LOO_ALL_TRAIN_DATA\"+str(k+1)+\".npy\", mmap_mode='c')\n",
    "    train_label = np.load(\"data_dump/\"+\"LOO_ALL_TRAIN_LABEL\"+str(k+1)+\".npy\", mmap_mode='c')\n",
    "    \n",
    "    for m in [0.02]:\n",
    "        big_test, small_test, big_label, small_label = train_test_split(test_data, test_label, test_size=m, random_state=42, shuffle=True)\n",
    "        \n",
    "        print(small_label.shape)\n",
    "        X_f = np.vstack((train_data, small_test))\n",
    "        y_f = np.vstack((train_label, small_label))\n",
    "        m_model, acc, f1 = _model(X_f, big_test, y_f, big_label, epoch)\n",
    "        record_acc.append(acc)\n",
    "        record_f1.append(f1)\n",
    "    \n",
    "    final_record_acc.append(record_acc)\n",
    "    final_record_f1.append(record_f1)\n",
    "    acc = sum(record_acc) / len(record_acc)\n",
    "    f1 = sum(record_f1) / len(record_f1)\n",
    "    print(\"Accuracy for participant \", N ,\"is: \", acc, \" and f1: \", f1)\n",
    "    if max_accuracy > acc:\n",
    "        max_accuracy = acc\n",
    "    total_acc += acc\n",
    "    total_f1 += f1\n",
    "        \n",
    "print(\"Leave one out avg. accuracy is: \", total_acc/N, \" F1 is: \", total_f1/N)\n",
    "print(\"\\n\\n\")\n",
    "print(final_record_acc)\n",
    "print(final_record_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c784a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = sum(record_acc) / len(record_acc)\n",
    "f1 = sum(record_f1) / len(record_f1)\n",
    "print(\"Accuracy for participant \", N ,\"is: \", acc, \" and f1: \", f1)\n",
    "if max_accuracy > acc:\n",
    "    max_accuracy = acc\n",
    "total_acc += acc\n",
    "total_f1 += f1\n",
    "print(\"Leave one out avg. accuracy is: \", total_acc/N, \" F1 is: \", total_f1/N)\n",
    "print(\"\\n\\n\")\n",
    "print(final_record_acc)\n",
    "print(final_record_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()\n",
    "m_model.export(\"models/tst_loo_6_\"+str(N)+\"_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0cf01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
